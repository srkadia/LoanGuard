{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LoanGuard: Model Training** 🚀\n",
    "\n",
    "## **Objective**\n",
    "This notebook focuses on building and evaluating machine learning models for **loan default prediction** using Lending Club data. The goal is to create a robust model that can accurately classify borrowers into **\"Fully Paid\"** and **\"Charged Off\"** categories, helping lenders make informed decisions.\n",
    "\n",
    "## **Models to be Implemented**\n",
    "- ✔️ **Baseline Models**: Logistic Regression, Naive Bayes, K-Nearest Neighbors (KNN)\n",
    "- ✔️ **Tree-Based Models**: Decision Tree, Random Forest, XGBoost, LightGBM, CatBoost\n",
    "- ✔️ **Artificial Neural Network (ANN)**: Deep learning-based approach for capturing complex patterns\n",
    "- ✔️ **Stacking Ensemble**: Combining multiple models for better predictive power\n",
    "\n",
    "## **Expected Outcome**\n",
    "By the end of this notebook, we aim to have a **high-performing loan default prediction model** that balances **accuracy, precision, and recall**, ensuring minimal false negatives (misclassified defaulters).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1. **Loading Libs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import hvplot.pandas\n",
    "import time\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, \n",
    "    confusion_matrix, classification_report, ConfusionMatrixDisplay, RocCurveDisplay\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "# NN\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Load and Preprocess Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and test data\n",
    "df_train = pd.read_csv('../data/processed/train.csv')\n",
    "df_test = pd.read_csv('../data/processed/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_target(df):\n",
    "    \"\"\"Encode loan_status: Fully Paid -> 0, Charged Off -> 1\"\"\"\n",
    "    df['loan_status'] = df['loan_status'].map({'Fully Paid': 0, 'Charged Off': 1})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = encode_target(df_train)\n",
    "df_test = encode_target(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X_train = df_train.drop(columns=['loan_status'])\n",
    "y_train = df_train['loan_status']\n",
    "X_test = df_test.drop(columns=['loan_status'])\n",
    "y_test = df_test['loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define Model Evaluation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Traditional Model Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, true, pred, train=True):\n",
    "    clf_report = pd.DataFrame(classification_report(true, pred, output_dict=True))\n",
    "    \n",
    "    if train:\n",
    "        print(f\"{model_name} - Train Result:\\n================================================\")\n",
    "    else:\n",
    "        print(f\"{model_name} - Test Result:\\n================================================\")\n",
    "    \n",
    "    print(f\"Accuracy Score: {accuracy_score(true, pred) * 100:.2f}%\")\n",
    "    print(\"------------------------------------------------\")\n",
    "    print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "    print(\"------------------------------------------------\")\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix(true, pred)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Neural Network Evaluation Function and Learning Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_neural_network(true, pred, train=True):\n",
    "    clf_report = pd.DataFrame(classification_report(true, pred, output_dict=True))\n",
    "    \n",
    "    if train:\n",
    "        print(\"ANN - Train Result:\\n================================================\")\n",
    "    else:\n",
    "        print(\"ANN - Test Result:\\n================================================\")\n",
    "    \n",
    "    print(f\"Accuracy Score: {accuracy_score(true, pred) * 100:.2f}%\")\n",
    "    print(\"------------------------------------------------\")\n",
    "    print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n",
    "    print(\"------------------------------------------------\")\n",
    "    print(f\"Confusion Matrix: \\n{confusion_matrix(true, pred)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_evolution(hist):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    ax1.plot(hist.history['loss'], label='Training Loss', color='red', linestyle='--', marker='o')\n",
    "    ax1.plot(hist.history['val_loss'], label='Validation Loss', color='red', linestyle='-', marker='s')\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\", color='red')\n",
    "    ax1.tick_params(axis='y', labelcolor='red')\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(hist.history['AUC'], label='Training AUC', color='blue', linestyle='--', marker='o')\n",
    "    ax2.plot(hist.history['val_AUC'], label='Validation AUC', color='blue', linestyle='-', marker='s')\n",
    "    ax2.set_ylabel(\"AUC Score\", color='blue')\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "    ax2.legend(loc=\"upper right\")\n",
    "\n",
    "    plt.title(\"Model Training Performance (Loss & AUC)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = X_train.shape[1]\n",
    "num_labels = 1\n",
    "\n",
    "def nn_model(hp):\n",
    "    input_dim = num_columns\n",
    "    inp = tf.keras.layers.Input(shape=(input_dim,))\n",
    "    x = BatchNormalization()(inp)\n",
    "    x = Dropout(hp.Float('dropout_1', 0.0, 0.5, step=0.1))(x)\n",
    "    \n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        x = Dense(hp.Int(f'units_{i}', min_value=64, max_value=256, step=32), activation='relu', kernel_initializer='he_normal')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(hp.Float(f'dropout_{i+2}', 0.0, 0.5, step=0.1))(x)\n",
    "    \n",
    "    output_activation = 'sigmoid' if num_labels == 1 else 'softmax'\n",
    "    x = Dense(num_labels, activation=output_activation, dtype=\"float32\")(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    optimizer = Adam(hp.Float('learning_rate', 1e-5, 1e-2, sampling='LOG'))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name='AUC')])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store scores\n",
    "scores_dict = {}\n",
    "model_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Traditional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, solver='saga'),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"XGBoost\": xgb.XGBClassifier(),\n",
    "    \"LightGBM\": LGBMClassifier(device=\"gpu\"),\n",
    "    \"CatBoost\": CatBoostClassifier(task_type=\"GPU\", verbose=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grids for key models\n",
    "param_grids = {\n",
    "    \"Random Forest\": {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"max_depth\": [None, 10, 20, 30],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "        \"max_depth\": [3, 6, 9],\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "    },\n",
    "    \"LightGBM\": {\n",
    "        \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "        \"num_leaves\": [31, 50, 100],\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform RandomizedSearchCV for broad search\n",
    "best_models = {}\n",
    "for model_name, param_grid in param_grids.items():\n",
    "    model = models[model_name]\n",
    "    random_search = RandomizedSearchCV(model, param_grid, cv=5, scoring='roc_auc', n_iter=10, n_jobs=-1, random_state=42)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    best_models[model_name] = random_search.best_estimator_\n",
    "    print(f\"✅ Best parameters for {model_name}: {random_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refining with GridSearchCV on the top performer (Optional)\n",
    "best_model_name = max(best_models, key=lambda name: roc_auc_score(y_test, best_models[name].predict_proba(X_test)[:, 1]))\n",
    "grid_search = GridSearchCV(best_models[best_model_name], param_grids[best_model_name], cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"🔍 Best fine-tuned parameters for {best_model_name}: {grid_search.best_params_}\")\n",
    "best_models[best_model_name] = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.update(best_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if name not in param_grids:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Predictions for evaluation\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluate performance\n",
    "    evaluate_model(name, y_train, y_train_pred.round(), train=True)\n",
    "    evaluate_model(name, y_test, y_test_pred.round(), train=False)\n",
    "\n",
    "    # Store accuracy results\n",
    "    model_results[name] = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    # Compute and store ROC AUC Scores\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        train_roc_auc = roc_auc_score(y_train, model.predict_proba(X_train)[:, 1]) * 100\n",
    "        test_roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]) * 100\n",
    "    else:\n",
    "        train_roc_auc = roc_auc_score(y_train, model.predict(X_train)) * 100\n",
    "        test_roc_auc = roc_auc_score(y_test, y_test_pred) * 100\n",
    "    \n",
    "    scores_dict[name] = {\n",
    "        'Train': train_roc_auc,\n",
    "        'Test': test_roc_auc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train).astype(np.float32)\n",
    "X_test = np.array(X_test).astype(np.float32)\n",
    "y_train = np.array(y_train).astype(np.float32)\n",
    "y_test = np.array(y_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    nn_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='../artifacts',\n",
    "    project_name='ann_tuning'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=20, \n",
    "    validation_data=(X_test, y_test), \n",
    "    batch_size=32, \n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=5\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256  # Larger batch for better GPU utilization\n",
    "\n",
    "# optimized data loading\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "with tf.device('/GPU:0'):  # Force training on GPU\n",
    "    history = best_model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=50,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "        ],\n",
    "        verbose=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "evaluate_neural_network(y_train, y_train_pred.round(), train=True)\n",
    "\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "evaluate_neural_network(y_test, y_test_pred.round(), train=False)\n",
    "\n",
    "# Compute ROC AUC Scores\n",
    "scores_dict[\"ANN\"] = {\n",
    "    \"Train\": roc_auc_score(y_train, best_model.predict(X_train)) * 100,\n",
    "    \"Test\": roc_auc_score(y_test, best_model.predict(X_test)) * 100,\n",
    "}\n",
    "\n",
    "model_results[\"ANN\"] = accuracy_score(y_test, y_test_pred.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress of Artifical Neural Network\n",
    "plot_learning_evolution(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(scores_dict)\n",
    "scores_df.hvplot.barh(\n",
    "    width=700, height=500, \n",
    "    title=\"Train vs Test ROC Scores of ML Models\", xlabel=\"ROC AUC Score (%)\", \n",
    "    alpha=0.6, legend='top'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC AUC scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=scores_df, palette=\"viridis\")\n",
    "plt.title(\"Comparison of ROC AUC Scores Across Models\", fontsize=14)\n",
    "plt.xlabel(\"Model\", fontsize=12)\n",
    "plt.ylabel(\"ROC AUC Score (%)\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(50, 100)  # Set a reasonable range for better visibility\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend([\"Train\", \"Test\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Comparison & Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(list(model_results.items()), columns=[\"Model\", \"Accuracy\"])\n",
    "results_df = results_df.sort_values(by=\"Accuracy\", ascending=False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Visualization of Model Performance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=results_df[\"Accuracy\"], y=results_df[\"Model\"], palette=\"viridis\", hue=results_df[\"Model\"], legend=False)\n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.ylabel(\"Model\")\n",
    "plt.title(\"Model Performance Comparison\")\n",
    "plt.xlim(0, 1)\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# Display the accuracy values on the bars\n",
    "for index, value in enumerate(results_df[\"Accuracy\"]):\n",
    "    plt.text(value + 0.01, index, f\"{value:.2%}\", va=\"center\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Saving the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_results[\"ANN\"]\n",
    "os.makedirs(\"../artifacts\", exist_ok=True)\n",
    "for name, model in best_models.items():\n",
    "    filename = f\"../artifacts/{name.replace(' ', '_')}_tuned.pkl\"\n",
    "    joblib.dump(model, filename)\n",
    "    print(f\"✅ Saved tuned {name} to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ANN model in HDF5 format\n",
    "os.makedirs(\"../artifacts\", exist_ok=True)\n",
    "best_model.save(\"../artifacts/ann_best_model.h5\")\n",
    "print(\"✅ Tuned ANN Model Saved Successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
